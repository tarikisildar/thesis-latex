% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Literature Review}\label{chapter:literaturereview}
The literature review presented in this chapter provides a comprehensive exploration of the key concepts,
technologies, and challenges in \acp{AV} and teleoperation. This review begins by establishing a
foundational understanding of \ac{AV} technology, tracing its evolution through the SAE automation levels.
While most consumer vehicles currently operate at Level 2 automation, industry leaders such as BMW and Waymo
are pushing the boundaries towards higher levels of autonomy \cite{bmw2024} \cite{evmagazine2024}.

The chapter then delves into the complexities of \ac{AV} architecture, highlighting the sophisticated
integration of hardware and software components that comprise the autonomous driving stack. Although end-to-end learning
approaches exist \cite{e2e}, the modular architecture remains predominant due to its interpretability and adaptability
across diverse scenarios \cite{codevilla2019limitations}. Each module within this stack, from perception to control,
is crucial in enabling safe and efficient vehicle operation.

A significant portion of the review is dedicated to sensor technologies and data processing, which form the backbone of
environmental perception in \acp{AV} \cite{feng2020deep,el-sheimy2020sensorfusion}. The discussion encompasses
the diverse array of sensors employed in modern \acp{AV}, including LiDAR for precise 3D mapping, radar for reliable object
detection in adverse conditions, and cameras for rich visual information capture. The EDGAR research vehicle at TUM is presented
as an exemplar of this multi-sensor approach.

The review then transitions to teleoperation concepts, tracing their evolution from early implementations of direct control to more sophisticated
Remote Assistance approaches \cite{kay2024sharedcontrol,corridor,hosseini2024collaborative,feiler2023perception}. Particular attention is
given to Perception Modification as a promising solution for addressing specific \ac{AV} challenges without requiring operators
to make complete control assumptions \cite{Feiler2021ThePM,Brecht}.

Visualization challenges are explored in depth, addressing both the technical hurdles of data integration and the human factors considerations
in interface design \cite{sworder1999performance,Gnatzig}. The large volumes of data generated by \acp{AV} require careful consideration of methods for
processing, transmission, and presentation. The effectiveness of visualization systems is linked to human cognitive capabilities,
requiring a balance between maintaining \ac{SA} and managing mental workload \cite{wickens2008multiple}.
The chapter concludes by examining industry solutions and showcasing commercial implementations such as Waymo's Fleet Response Interface and
Zoox's TeleGuidance System \cite{waymo2024fleetresponse,zoox2024teleguidance}. These examples demonstrate different approaches for integrating
multiple data streams for effective teleoperation. The ToD Visual 2.0 platform, developed at TUM \cite{Schimpe}, is presented as an academic approach to these challenges, providing a foundation for further research and development.

Through this comprehensive review, the chapter establishes the context for the research presented in this thesis,
identifying gaps in current approaches and setting the stage for the development and evaluation of an advanced
\ac{HMI} for the teleoperation of \acp{AV}, with a specific focus on enhancing \ac{SA} for Perception Modification tasks.

\section{Overview of Autonomous Vehicle Technology}

The Society of Automotive Engineers (SAE) International has defined six levels of
driving automation, as shown in figure 2.1 in their J3016 standard, which has become
the industry's most widely accepted classification system. These levels range from 0
(no automation) to 5 (full automation), providing a clear framework for understanding
the capabilities of \acp{AV} \cite{sae2021}.

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/SAE.png}
    \centering
    \caption{SAE Levels of Automation \cite{sae2021}}
    \label{fig:SAE}
\end{figure}

As of 2024, most advanced consumer vehicles operate at Level 2, with some manufacturers pushing towards Level 3 capabilities.
For example, BMW has recently become the first carmaker to receive approval for combining both Level 2 and Level 3 autonomous driving systems in a single vehicle.
The new BMW 7 Series now offers the BMW Highway Assistant (Level 2) and the BMW Personal Pilot L3 (Level 3), marking a significant step in automated driving technology \cite{bmw2024}.

At the forefront of \ac{AV} technology, companies like Waymo
are making significant strides towards Level 4 autonomy. Waymo has achieved
Level 4 autonomy in pilot areas, offering fully autonomous rides without safety
drivers in cities like San Francisco, Phoenix, and Austin \cite{evmagazine2024}.

 Achieving higher SAE levels of autonomy requires a sophisticated autonomous driving stack,
 which is composed of several vital modules that work together to perceive the environment,
 make decisions, and control the vehicle. The stack typically follows a modular software
 architecture that is shown in Figure \ref{fig:AVStack} integrates hardware and software components to enable safe and
 efficient vehicle operation.
 Modular software architecture is the predominant approach in \ac{AV} development, as it allows for separation of the
 modules and solve the issues separately. It startst with the perception module, which processes sensor data to understand
 the environment. Including mapping, localization, object detection, and tracking. The perception module feeds into the prediction
 where the vehicle anticipates the behavior of other road users and objects. The planning module then generates a trajectory for local
 and global scale. Finally, the control module executes the planned trajectory by sending commands to the vehicle's actuators. It is important
 to note that the perception module is the most critical part of the stack, as it provides the foundational data for all other modules. Any errors
 in perception can propagate through the entire system, leading to incorrect decisions and unsafe driving behavior.

\begin{figure}[h]
    \includegraphics[scale=0.14]{figures/AVStack.png}
    \centering
    \caption{Autonomous Vehicle Technology Stack
    from the lecture slides of "Software Engineering For Autonomous Driving" course, TUM, 2024} %todo: add source from the lecture
    \label{fig:AVStack}
\end{figure}

While some research has explored end-to-end learning, where \ac{DL} models directly map sensor inputs
to control outputs without modular decomposition \cite{codevilla2019limitations}, this approach alone is often
insufficient for achieving higher levels of autonomy. End-to-end systems can struggle with interpretability
and adaptability across diverse driving scenarios \cite{e2e}. It is an active research area and some
of the mentioned issues can be solved with modular end-to-end learning \cite{nvidia2022diffstack} and also
it is expected to have a better performance on end-to-end methods with the improving hardware and datasets overtime \cite{e2e}.

For a system to reach higher SAE levels—particularly Level 4 or 5—all of
these modules must function reliably across various environments and conditions.
Each component plays a critical role in ensuring that the vehicle can navigate complex urban
environments safely and efficiently while responding to dynamic changes in real-time.
On the \acp{AV} with an automation level of 3 or lower, the driver must be ready to take over the control
when the system reaches its limits. This is called the "Handover" process.
But for the higher levels of automation, the vehicle must be able to handle the edge cases without an onboard human driver.
For that reason, we have teleoperation for the the cases where one or more module fails to deal the edge cases.
Teleoperation serves as a bridge between current autonomous driving capabilities and fully autonomous operation.
It allows vehicles to complete their missions even when their onboard systems encounter limitations.
For instance, if an \ac{AV} encounters a situation outside its \ac{ODD} \cite{iso34503},
such as unclear road markings or interactions with law enforcement, it can pull over safely and request human
intervention through teleoperation.
This technology is essential for ensuring that Level 4 and 5 \acp{AV} can operate reliably in real-world conditions
and is expected to play a key role in the widespread adoption of these \acp{AV}.


\subsection{Types of Data used in Autonomous Vehicles} \label{subsection:sensors}
Autonomous vehicles (\acp{AV}) rely on a diverse array of sensors and data sources to perceive their environment and gather the necessary
information for safe and efficient navigation. These sensors provide raw data that is
processed by the vehicle's perception system to understand
its surroundings, detect obstacles, and make real-time
decisions \cite{thrun2006stanley}.
In this thesis, we refer to this unprocessed data as
"Raw data" or "Raw sensory data." Additionally to the sensory data,
offline HD maps serve as a critical source of pre-processed environmental information,
complementing real-time sensor data with detailed static information about road layouts
and traffic elements \cite{bansal2018hdmaps,zhu2021hdmaps}.

\textbf{LiDAR (Light Detection and Ranging):}
    LiDAR sensors use laser pulses to measure distances between the vehicle and surrounding objects.
    By emitting laser beams and measuring the time it takes for them to reflect back, LiDAR creates
    a detailed 3D map of the environment. This technology is particularly useful for detecting objects'
    shapes, sizes, and positions with high accuracy, even in low-light conditions \cite{levinson2011towards}.
    LiDAR is widely used in \ac{AV} systems due to its precision in mapping the surrounding area.

\textbf{Radar}:
    Radar sensors use radio waves to detect objects and measure their speed and distance from the vehicle.
    Unlike LiDAR, radar is less affected by adverse weather conditions such as rain or fog, making it a
    reliable sensor for detecting moving objects like other vehicles or pedestrians \cite{patole2017automotive}.
    Radar is often used for functions like adaptive cruise control and collision avoidance.

    \textbf{Cameras}:
    Cameras capture visual data from the environment, providing rich information about road signs,
    lane markings, traffic lights, and other vehicles. \acp{AV} typically use multiple cameras positioned
    around the vehicle to achieve a 360-degree view \cite{geiger2012we}. Cameras are crucial for tasks requiring
    high-resolution visual data, such as object classification and scene understanding.

    \textbf{Ultrasonic Sensors}:
    Ultrasonic sensors are commonly used for short-range detection tasks like parking assistance or detecting nearby
    obstacles at low speeds. These sensors emit sound waves and measure their reflections to detect objects within
    a few meters of the vehicle \cite{zhang2018ultrasonic}.

    \textbf{GPS (Global Positioning System)}:
    GPS provides location data by triangulating signals from satellites. While GPS alone may not offer the precision
    required for autonomous driving, it is often combined with other localization methods such as SLAM
    (Simultaneous Localization and Mapping) to improve accuracy \cite{thrun2005slam}.

    \textbf{IMU (Inertial Measurement Unit)}:
    The IMU measures the vehicle's acceleration and angular velocity using accelerometers and gyroscopes.
    This data helps track the vehicle's movement and orientation in real time, contributing to accurate
    localization when combined with GPS data \cite{madgwick2011imu}.

    \textbf{Offline HD Maps:}
    Offline HD maps provide detailed information about road layouts, traffic signs, lane markings, and other static features of the driving environment. These maps enhance \ac{SA} by offering context beyond what onboard sensors can perceive in real time. HD maps are particularly useful for improving localization accuracy when combined with GPS and IMU data \cite{bansal2018hdmaps}. They also assist in path planning by providing precise information about road geometry, which is critical for autonomous navigation \cite{zhu2021hdmaps}.

We referenced a subset of TUM's EDGAR research vehicle's sensor setup as our basis.
EDGAR is equipped with 10 camera sensors, 4 LIDAR sensors (long and short-range), 6 RADAR sensors as well as GPS, IMU (Inertial Measurement Unit) and microphones \cite{tum2023edgar}.

\subsection{Perception Systems in Autonomous Vehicles}
Perception systems are an essential component of \acp{AV}, responsible for interpreting the
vehicle's surroundings and enabling safe navigation. They are the basis of the modular software stack from \ref{fig:AVStack}.
These systems process data from sensors such as LiDAR,
radar, and cameras to detect and classify objects, track their movements, and localize the vehicle within
its environment \cite{liu2018perception}. The perception system builds a comprehensive model of the environment,
which is then used by other modules—such as prediction, planning, and control—to make driving decisions.

This thesis will refer to "Perception Data" as the processed output from the vehicle's perception system. This includes information about detected objects (e.g., vehicles, pedestrians), their classifications, positions, and trajectories. The perception system also generates high-level semantic information such as lane markings, traffic signs, and road boundaries. This data enables \acp{AV} to understand their environment and make informed decisions.

We utilize Autoware, an open-source software stack designed for autonomous driving systems for all perception-related tasks in this project. Autoware provides a comprehensive set of tools for processing sensor data and generating perception outputs \cite{kato2018autoware}. It includes object detection, classification, tracking, and localization modules using sensors such as LiDAR, radar, and cameras. By leveraging Autoware's robust perception capabilities, we ensure that our system can handle complex environments while maintaining flexibility for future improvements.

Autoware also supports sensor fusion techniques that combine data from multiple sensors to improve accuracy and reliability. This is especially important when dealing with noisy or incomplete sensor data. For instance, Autoware's fusion algorithms can combine LiDAR point clouds with radar measurements to enhance object detection
performance in adverse weather conditions where cameras may struggle.
\section{Teleoperation Concepts}

Teleoperation is an essential fallback solution for \acp{AV},
enabling a human operator to remotely assist or take control of a vehicle when
its automated systems encounter situations beyond their \ac{ODD}
or face disengagements. These scenarios often arise due to complex or unforeseen edge cases,
such as sensor malfunctions, unanticipated obstacles, or ambiguous road conditions.
The system ensures that the \ac{AV} can resume its mission safely and efficiently \cite{Brecht}.

Several teleoperation concepts have been developed to address different aspects of remote vehicle control, each with its own strengths and limitations. These concepts can be broadly categorized into Remote Driving and Remote Assistance with subcategories. Each approach targets specific challenges in teleoperation, such as latency, \ac{SA}, and operator workload.

\subsection{Remote Driving}
Remote Driving is one of the earliest teleoperation concepts,
where a human operator takes full control of the vehicle's
dynamic driving tasks (DDT), including steering, acceleration,
and braking. This concept has been explored for more than two decades.
Early research by Sworder et al. \cite{sworder1999performance} (1999) demonstrated the feasibility
of teleoperating vehicles using direct control methods, where
operators manually control vehicles based on live video feeds and
sensor data transmitted from the vehicle to a remote station.
This method, known as Direct Control, has been widely studied
and implemented in various industries since then \cite{Gnatzig,chucholowski2014teleoperated,Tang}.

Despite its long history, Direct Control remains relevant
today due to its simplicity and directness. In this approach,
the operator receives real-time video and sensor data from the
vehicle and sends back control commands such as steering angles
or velocity adjustments. However, this method is highly sensitive
to network conditions—particularly latency and bandwidth
limitations—which can lead to reduced \ac{SA} for
the operator and delayed reactions to dynamic situations \cite{Gnatzig}.
Latency is a critical factor in teleoperation; studies have shown
that latencies greater than 10 milliseconds can result in degraded
performance in high-speed or complex environments \cite{chucholowski2014teleoperated}.
Under fluctuating network conditions—such as handovers
between cellular towers or signal degradation due to obstacles—latency
can increase unpredictably, leading to delayed responses that
compromise safety \cite{neumeier2023feasibility}.

In recent years, several companies have implemented Direct Control
in real-world applications despite these challenges. For example,
Fernride, a Munich-based company, has successfully deployed
teleoperation technology for logistics operations. Fernride's
platform allows remote operators to control semi-autonomous
trucks in logistics yards using Direct Control methods.
In their pilot project with DB Schenker and KAMAG,
teleoperators remotely controlled trucks for yard shunting
tasks—moving trailers around loading docks—demonstrating that
teleoperation can be safely integrated into existing logistics
processes \cite{fernride2023}. The success of this project
highlights the potential of Direct Control in controlled
environments like logistics yards where network conditions are stable.

However, while Direct Control has proven effective in specific use cases
such as logistics yards or industrial environments with
controlled network conditions, it faces significant challenges when
applied to more complex or dynamic environments like urban roads.
The sensitivity of Direct Control to network latency makes it less
suitable for scenarios where real-time decision-making is critical for
safety. To mitigate these challenges, modern teleoperation systems
often incorporate additional safety measures or alternative methods
that reduce reliance on real-time control inputs.

One such alternative is Shared Control, where the vehicle retains
some level of autonomy while the operator provides the control inputs as in the Direct Control.
In this approach, if an operator's input does not meet
certain safety criteria—such as proximity to other vehicles—the
vehicle's onboard systems can override those commands to prevent
collisions \cite{kay2024sharedcontrol}. This approach reduces
cognitive load on the operator while ensuring that safety-critical
decisions are still handled by the vehicle's autonomous systems.

\subsection{Remote Assistance}
Remote Assistance focuses on providing high-level guidance or support
to specific functions of the AV without taking full control of the DDT.
This method is often used in scenarios where the AV encounters an
edge case that prevents it from continuing its mission autonomously
but does not require complete human intervention \cite{Brecht}.

In Waypoint Guidance, for example, operators provide waypoints or general
directions for the vehicle to follow while leaving detailed path planning
and execution to the AV's onboard systems \cite{corridor}. This reduces
operator workload but may lead to stop-and-go behavior if communication
between the operator and vehicle is not seamless.

Another promising approach is Collaborative Planning, where operators
work with the AV's planning system by selecting from a set of pre-generated
path suggestions based on sensor data. This method allows operators to focus
on decision-making rather than low-level control tasks \cite{hosseini2024collaborative}.
Collaborative planning improves efficiency but requires robust sensor fusion and reliable
perception data from the AV.
\subsection{Perception Modification}

Perception Modification is one of the most promising teleoperation
concepts within Remote Assistance category. Designed to address situations where an \ac{AV}
encounters perception errors that prevent it
from continuing its mission. These errors may arise from false
positives or misclassifications in the vehicle's perception system,
such as detecting an object that is not truly obstructing the path or
misinterpreting environmental features like lane markings or road signs.
In such cases, Perception Modification allows a remote operator to
intervene by modifying some elements of perception to correct \ac{AV}'s interpretation,
enabling it to resume its journey \cite{Feiler2021ThePM,feiler2023perception}.

In this concept, the operator has access to both raw sensor data
(e.g., LiDAR point clouds or camera images) and processed perception
data (e.g., object classifications or environmental models).
By reviewing this data, operators can identify discrepancies
between what the vehicle perceives and what is present in its
surroundings. For example, if a false-positive detection is blocking
the vehicle's path—such as a plastic bag being detected as a solid
obstacle—the operator can modify or correct these perceptions by
marking the area as drivable. This correction enables the AV to
continue its mission without requiring complete manual control \cite{feiler2023perception}.

The Perception Modification concept is beneficial because
it reduces the cognitive load on the operator compared to direct control methods.
Instead of managing all aspects of driving, operators focus solely on correcting
specific perception errors. This approach allows for more efficient human
intervention while minimizing the risk of operator fatigue or error due to
high mental workload \cite{Brecht}. Additionally, Perception Modification can
be extended beyond object detection corrections; operators could also modify other
aspects of the environmental model, such as lane boundaries or drivable space predictions.

As Feiler and Diermeyer \cite{Feiler2021ThePM,feiler2023perception} define it, the implementation of
Perception Modification involves integrating a  Human-Machine Interface (HMI) that provides operators with real-time access to both raw sensor data and processed perception data. The HMI allows operators to
mark areas as drivable or ignore specific detections deemed irrelevant or incorrect. The vehicle's planning module receives this modified perception data and generates a new trajectory based on these corrections.

In their comparative study of teleoperation concepts, Brecht et al. \cite{Brecht} found
that Perception Modification imposes less cognitive load on operators than
Direct Control while still enabling efficient resolution of disengagements.
The study also highlighted that Perception Modification is highly effective in
situations where false-positive detections or indeterminate objects hinder an AV's progress.
However, it may not be suitable for all disengagement scenarios—particularly those
involving complex trajectory planning failures or situations requiring immediate manual intervention.

\section{Environmental Data Visualization for Autonomous Vehicles}
Environmental data visualization plays a fundamental role in \ac{AV} systems,
particularly in teleoperation scenarios where human operators must understand and interpret
complex sensor data to make critical decisions.
Visualizing environmental data is crucial for two reasons: First, it enables operators to
maintain high \ac{SA} during
teleoperation tasks \cite{Gnatzig}. Second, it allows for effective monitoring and verification
of the vehicle's perception system, essential for identifying and correcting potential errors
in the automated driving system \cite{feiler2023perception}.

\subsection{Challenges in Visualizing Multi-Sensor Data}\label{subsection:challengesmultisensor}
One of the primary challenges in environmental data visualization is integrating data from multiple
sensors into a cohesive representation of the environment. Modern \acp{AV} generate
massive amounts of data - up to 3-6 TB of raw data per hour of operation from their sensor suite \cite{kazhamiaka2021challenges}
. A single 4K camera operating at 30 frames per second can produce
approximately 5.4 TB per hour, while a typical AV may have 6-12 cameras along with other sensors \cite{visualcapitalist2024}
. This creates a significant challenge in data processing, transmission,
and visualization. Each sensor type has strengths and limitations, as defined in section \ref{subsection:sensors}.

The sensor fusion process combines these different data streams into a single, a unified environmental
model that is used by nearly all parts of the autonomous driving stack like perception system \cite{feng2020deep}
, localization system \cite{feng2020deep, el-sheimy2020sensorfusion}. It can also be used for the human operator interface.
However, visualizing this fused data presents several challenges:

\textbf{Real-Time Processing:} Processing and visualizing sensor data in real time requires significant computational resources. Delays in processing can lead to outdated information being presented to the operator, reducing \ac{SA} \cite{Gnatzig}.

\textbf{Accuracy vs. Latency:} High-fidelity visualizations may improve accuracy but come at the cost of increased latency. A balance between these two factors must be kept for effective teleoperation \cite{chucholowski2014teleoperated}.

Data overload is particularly sensitive in teleoperation scenarios, where operators must quickly understand and respond to complex environmental data while maintaining real-time awareness of the vehicle's situation. This requires careful consideration of which data to present and how to present it effectively without overwhelming the operator's cognitive capabilities.

\subsection{Human Factors in Perception Data Visualization}\label{subsection:humanfactors}
We consider two critical elements in teleoperation in the aspect of human factors: \ac{SA} and mental workload. These factors significantly influence an operator's ability to effectively control and monitor remote vehicles.

\textbf{Situational Awareness:} In teleoperation scenarios, achieving and maintaining \ac{SA} presents unique challenges due to the physical separation between the operator and the vehicle. Gnatzig et al. \cite{Gnatzig} shows that operators must rely entirely on sensor data and interface representations to understand the remote environment, making the quality and presentation of this information critical for effective operation.  As Endsley \cite{endsley1995toward} states, "Even the
best-trained decision makers will make the wrong decisions if they have inaccurate or incomplete SA.".

\textbf{Mental Overload:} Presenting all sensor data simultaneously can overwhelm operators with too much information.
Operators must process and integrate information from multiple sources while maintaining awareness of critical events, which can lead to increased mental workload \cite{wickens2008multiple}.

\section{Existing Approaches to Visualizing Sensor Data}
The visualization of sensor data and perception outputs is crucial for developing and monitoring \acp{AV}. Various approaches have been developed by both industry and academia to address the challenges of presenting complex environmental data effectively. This section examines existing visualization solutions, starting with industry standards and progressing to more specialized approaches.
\subsection{Industry Standards}
Major \ac{AV} companies have developed sophisticated operator interfaces to support remote monitoring and intervention when autonomous systems face complex or ambiguous scenarios. These interfaces are designed to provide comprehensive environmental awareness by integrating real-time sensor data, perception outputs,
and vehicle behavior into an accessible format. Below, we highlight examples of operator-focused visualization systems developed by leading companies.

Waymo's Fleet Response Interface \cite{waymo2024fleetresponse}, shown in figure \ref{fig:Waymo} is a prime example of a teleoperation tool designed for remote intervention. This interface provides a detailed 3D visualization of the vehicle’s surroundings, integrating LiDAR point clouds, camera feeds, and object detection data in real-time. Remote operators can monitor the vehicle’s perception system and provide guidance when necessary. For instance, operators can review live video feeds and rewind past footage to better understand complex situations such as construction zones or ambiguous road markings. The system allows operators to suggest lane changes or route adjustments while ensuring the Waymo Driver remains in complete vehicle control.

\begin{figure}
    \includegraphics[width=0.8\textwidth]{figures/waymo.png}
    \centering
    \caption{Waymo's Fleet Response Interface \cite{waymo2024fleetresponse}}
    \label{fig:Waymo}
\end{figure}

Zoox's TeleGuidance System \cite{zoox2024teleguidance} is another example of an operator interface designed for remote assistance. Like Waymo’s system, Zoox provides a separate view of the camera feed and a 3D environment model of the vehicle’s perception. This system allows the operator to do both waypoint guidance
 \cite{corridor} and also a subset of perception modification \cite{feiler2023perception} by object recategorization.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/zoox.png}
        \caption{Zoox's TeleGuidance System}
        \label{fig:Zoox}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/fernride.png}
        \caption{Fernride's Teleoperation Interface}
        \label{fig:Fernride}
    \end{subfigure}
    \caption{Comparison of Interfaces for Remote Assistance(a) and Direct Control(b)}
    \label{fig:TeleoperationComparison}
\end{figure}

As shown in \ref{fig:Zoox}, They also provide some perception information mixed in with the video feed.

Fernrides' strategy is different from the other two. They provide a direct
control interface for their teleoperation system. The operator can control the vehicle directly with a joystick and monitor the vehicle's surroundings with a 360-degree camera feed. This approach suits controlled environments like logistics yards, where real-time control is essential for efficient operations \cite{fernride2023}. Since their level of autonomy is lower than that of other examples, the need for perception visualization is also lower. Thus, they focus more on the raw sensory data representations.

\subsection{ToD Visual 2.0}\label{subsection:todvisual}
The software stack on which we are building our interfaces is based on ToD Visual 2.0, a teleoperation visualization software developed by the TUM Institute of Automotive Technology. This software is an improved version of ToD Visual \cite{Schimpe}, initially developed for the TUM EDGAR research vehicle. The new version introduces a modular structure, allowing for the easy integration of various visualization approaches and customization to meet specific use cases.ToD Visual 2.0 has already been showcased in public as part of the TUM Wiesn Shuttle event, where it was used to display sensory and perception data during the autonomous shuttle's operation in the challenging traffic conditions surrounding Munich's Oktoberfest \cite{adac2024wiesn, tum2024wiesn}. This real-world demonstration highlighted the software's capabilities in rendering complex sensor data and providing meaningful insights into vehicle behavior under extreme conditions. Although ToD Visual 2.0 is not yet open-sourced, plans are underway to make it publicly available. This will enable broader adoption and collaboration within the research and development community. The software stack provides robust capabilities for rendering sensory information, such as point clouds from LiDAR, camera images, and radar data. Additionally, it supports visualization of perception outputs, including:
\begin{itemize}
    \item Bounding boxes with color-coded classifications for object detection,
    \item Trajectory information for planned vehicle paths,
    \item Lane information derived from offline HD maps.
\end{itemize}

\begin{figure}
    \includegraphics[width=0.8\textwidth]{figures/tod_visual.png}
    \centering
    \caption{ToD Visual 2.0 Interface from Wiesn' Shuttle Event}
    \label{fig:ToDVisual}
\end{figure}

The interface before our contribution within this paper can be seen in figure \ref{fig:ToDVisual}.

For this thesis, we have taken ToD Visual 2.0 as a foundation and enhanced several components to align with our specific use case. These improvements include refining existing features and adding new functionalities tailored to support our Separate View and Integrated View approaches.


\subsection{Separate View}\label{section:separateview}
In this section, we define the Separate View visualization approach, which involves presenting 2D and 3D data in distinct windows or displays for the operator. This method allows operators to view the 2D data (e.g., camera images) alongside processed perception outputs and raw 3D data (e.g., object detections, lidar point clouds) in separate, clearly delineated spaces. By separating these views, operators can compare the raw data against the system's perception outputs to identify potential discrepancies, such as misclassifications or false positives.

The Separate View approach is particularly advantageous for teleoperation scenarios where \ac{SA} and perception verification are critical. Operators can use the 2D view to observe raw camera feeds for a direct visual understanding of the environment, while the 3D view provides a synthesized representation of the vehicle's perception system, including object bounding boxes, and trajectory predictions alongside with raw 3D data like LiDAR point clouds. This dual-view setup aids in identifying errors in the perception system and enables operators to make informed decisions when intervening.
\paragraph{Implementation in Our Work} Our implementation of the Separate View approach builds on a codebase called ToD-Visual 2.0, explained in depth in section \ref{subsection:todvisual}, which provides a foundation for developing advanced teleoperation interfaces.
It gives us the base ability to visualize raw sensor data and special visualizations for some of the perception output.
a parallel thesis project by El Alami \cite{yassinethesis} focuses on further developing this Separate View concept, exploring ways to optimize its usability and
implementation of visualization methods for required perception components. This collaboration ensures that the work benefits from iterative improvements and shared insights into the challenges and
opportunities associated with this visualization method.

\subsection{Integrated View Approaches}\label{section:integratedview}
We define the Integrated View approach as combining 2D and 3D data into a
unified visualization for the operator. According to Wickens' multiple resource theory \cite{wickens2008multiple},
humans have limited cognitive resources, and splitting attention across multiple displays can overload these resources,
reducing task performance and \ac{SA}. The Integrated View aims to mitigate these challenges by
merging raw sensor data with processed perception outputs into a single representation. This approach seeks to enhance
\ac{SA} and reduce cognitive overload by eliminating the need for operators to manage multiple displays.
However, achieving this integration is not without challenges. A key issue is the dimensionality disparity between the data sources:
visual data from cameras is inherently 2D, while perception data such as LiDAR point clouds exist in 3D.
Ultimately, all this information must be presented to the operator in a 2D format.
This section explores two primary methods for addressing this challenge:
Image Space Representation and 3D Space Representation.

\paragraph{Image Space Representation} One common approach is to project 3D information onto a
2D image space. For example, bounding box projections for object detection are
widely used in autonomous driving systems as can be seen in Zoox's TeleGuidance sytem
on figure \ref{fig:Zoox}. These projections overlay 3D object
detections onto 2D camera images, providing operators with a simplified view of
the environment. Florea et al. \cite{Florea} extend this concept by using semantic segmentation on
3D point cloud data and projecting the results onto 2D images for improved
visualization. Despite its simplicity, this approach has significant limitations.
Critical depth information is lost when 3D data is projected onto a 2D plane. This makes
it difficult to accurately visualize overlays like bounding boxes, trajectories, or
lane markings in terms of their accurate spatial positions. These can be mitigated by
representing them in a way that fits into the perspective view. There are also occlusion
challenges, as the objects closer to the camera may obscure those further away, leading
to incomplete or misleading visualizations.

\begin{figure}
    \includegraphics[width=0.8\textwidth]{figures/perceptionMod.png}
    \centering
    \caption{The interface used in the original Perception Modification paper \cite{feiler2023perception}}
    \label{fig:PerceptionMod}
\end{figure}
\paragraph{3D Space Representation} This approach involves maintaining the environmental
data in 3D space while providing methods to integrate 2D camera feeds. This can be
achieved either through projections onto basic geometric shapes, as demonstrated by
Feiler and Diermeyer \cite{feiler2023perception}, and shown in the figure \ref{fig:PerceptionMod}, or through a comprehensive 3D reconstruction of the surroundings using camera and depth data
. While geometric projection methods offer simplicity, they often encounter issues with collision of perception data points
that do not align with the projected locations. The creation of accurate 3D reconstructions, though more challenging to implement,
provides a more complete and spatially accurate representation of the environment. We will refer to this
comprehensive approach as the "Integrated View" throughout this thesis, as it offers the most promising solution for
combining both raw sensor data and perception outputs in a unified, spatially coherent representation. We will explore
two approaches for implementing this Integrated View. The first approach involves using \ac{NeRF} to rendering
realistic 3D reconstructions of the environment. The second approach focuses on depth completion techniques to fuse the sensory data
to have a more complete 3D representation of the environment.

\subsection{NeRF-based rendering methods}
\ac{NeRF}, first introduced by Mildenhall et al. \cite{mildenhall2020nerf}, represent scenes as continuous volumetric functions using neural networks. A ac{NeRF} takes a 5D input coordinate (the spatial location $(x,y,z)$ and viewing direction $(\theta,\phi)$) and outputs the volume density and view-dependent emitted radiance at that spatial location. This allows photorealistic novel view synthesis by optimizing the neural network using input images with known camera poses.

In the context of autonomous driving, ac{NeRF} have been particularly effective for static perception tasks such as map construction due to their inherent property of multi-view consistency. Multiple works have applied ac{NeRF} to automotive data, focusing initially on static scenes where the environment remains unchanged \cite{snerf2023}. These approaches typically employ anti-aliased positional embeddings to handle scale variations essential for large-scale scenes.

However, autonomous driving scenarios inherently involve dynamic objects, presenting significant challenges for traditional ac{NeRF} approaches. Recent works have proposed methods to handle dynamic scenes to address this limitation. NVIDIA's EmerNeRF introduces a self-supervised approach that decomposes scenes into static, dynamic, and flow fields \cite{yang2023emernerf}. This decomposition emerges from self-supervision, enabling the model to learn from general, in-the-wild data sources without requiring ground truth annotations or external models for dynamic object segmentation.

Similarly, Wayve's PRISM-1 significantly advances handling dynamic urban environments \cite{prism2024wayve}. It can capture complex scenes with multiple moving elements, including pedestrians, cyclists, and other vehicles while accounting for dynamic lighting conditions such as blinking traffic and brake lights. The system learns to separate static from dynamic elements in a self-supervised manner and implicitly tracks movements in the scene, matching them with the 3D geometry.

When considering ac{NeRF}-based methods for our Integrated View approach, several factors must be evaluated:

\begin{table}[h!]
    \centering
    \begin{tabular}{|p{7cm}|p{7cm}|}
    \hline
    \textbf{Advantages} & \textbf{Limitations} \\ \hline
    High-fidelity photorealistic reconstructions & Significant computational resources are required for training and rendering \\ \hline
    Comprehensive representation of both static and dynamic elements & Challenges in handling highly dynamic environments with rapid changes \\ \hline
    Natural handling of multi-view consistency & Need for precise camera pose information and multiple viewpoints \\ \hline
    \end{tabular}
    \caption{Comparison of Advantages and Limitations}
    \label{table:advantages_limitations}
\end{table}

These characteristics make ac{NeRF}-based methods a promising but challenging candidate for real-time teleoperation interfaces. While they offer superior visual quality and spatial consistency, their computational demands and real-time performance limitations must be carefully considered for practical applications. It is important to note that it is a novel approach that has rapidly improved in recent years.

\subsection{Depth Completion}

Depth completion addresses converting sparse and irregular depth data into dense, regular depth information. This process is particularly relevant for \acp{AV}, where LiDAR sensors provide accurate but sparse 3D measurements. Having complete depth information enables precise scene reconstruction, as the dense depth map allows for accurate projection of camera images onto the 3D space, creating a comprehensive view of the environment \cite{tang2023comprehensive}.

The KITTI depth completion benchmark has become the standard evaluation platform for these methods \cite{uhrig2017sparsity}, providing a dataset of over 93,000 depth maps with corresponding raw LiDAR scans and RGB images. The benchmark evaluates methods using metrics such as iRMSE (inverse root mean square error) and iMAE (inverse mean absolute error) to assess performance against ground truth data.

Recent advances in depth completion have explored various learning approaches. Supervised methods like GuideNet have demonstrated superior performance by effectively fusing image features at different encoder stages with sparse depth features \cite{tang2023guidenet}. The Convolutional Spatial Propagation Network (CSPN) achieves high accuracy with relatively fast runtime, making it suitable for real-time applications \cite{cheng2020cspn}. These supervised approaches, however, heavily rely on ground truth data, which can be costly and challenging to obtain in practical applications.

Self-supervised and unsupervised methods have emerged as promising alternatives that eliminate the dependence on ground truth data. Wong et al. introduced a self-supervised approach that leverages visual inertial odometry to complete depth maps without explicit supervision \cite{wong2020unsupervised}. Recent work by Li et al. explores using 3D perceptual features and multi-view geometry consistency to achieve high-precision depth completion without ground truth data \cite{li2023self}. However, the accuracy remains slightly inferior to supervised methods.

Several factors warrant consideration when considering depth completion for our Integrated View approach. The method provides accurate dense depth information and enables precise 3D scene reconstruction while being computationally more efficient than ac{NeRF}-based methods. Some implementations achieve real-time performance, making them suitable for teleoperation applications. However, the approach faces challenges such as dependence on LiDAR sparsity patterns and potential struggles in areas with high occlusion. Additionally, the method requires careful sensor calibration and synchronization to ensure accurate fusion of LiDAR and camera data.

These characteristics make depth completion a promising approach for real-time teleoperation interfaces, mainly when computational efficiency is a priority. Generating dense, accurate depth maps in real time provides a solid foundation for creating comprehensive environmental visualizations for teleoperators.

\section{User Experience and Interface Design for Autonomous Vehicles}
The design of effective user interfaces for \acp{AV} represents a critical challenge in the development of safe and reliable autonomous systems. As vehicles become increasingly automated, the interaction between humans and autonomous systems becomes more complex, particularly in scenarios requiring human intervention \cite{Kettwich}. While the ultimate goal of \acp{AV} is to minimize human involvement, research has shown that human operators will still be required for handling edge cases and complex scenarios that exceed the vehicle's autonomous capabilities \cite{mutzenich2021updating}.
\subsection{Human-Machine Interaction in Autonomous Vehicles}
\ac{HMI} in \acp{AV} focuses on creating interfaces that facilitate effective communication between the operator and the vehicle's systems. The primary goal is to ensure safe operation while maintaining an accessible user experience. This is particularly challenging in teleoperation scenarios, where operators must process complex information from multiple sources while maintaining high levels of \ac{SA} \cite{Georg}.
Modern HMI systems incorporate multiple interaction modalities to enhance operator performance and safety. Visual interfaces through \acp{GUI} serve as the primary information channel, while additional modalities such as auditory and haptic feedback provide complementary information \cite{kallioniemi2021enhancing}. This multi-modal approach helps distribute cognitive load across different sensory channels, potentially improving operator performance and reducing mental workload.
\subsection{User-Centered Design Principles for Vehicle Interfaces}
User-centered design principles highlight understanding user needs, expectations, and limitations while ensuring safety and performance. Following the EN ISO 9241-110 process, interface design consists of four key steps: understanding the context of use, specifying usage requirements, developing design solutions, and evaluating these solutions \cite{Georg}.

Kettwich et al. established seven critical requirements for teleoperation interfaces \cite{Kettwich}:

\begin{table}[h!]
    \centering
    \begin{tabular}{@{}p{4cm}p{10cm}@{}} % Remove padding with @{}
    \toprule
    \textbf{Category} & \textbf{Description} \\
    \midrule
    Features & The interface must provide necessary features to monitor automation, provide disturbance information, and support operators in resolving issues \\
    Information & The system must present necessary information effectively \\
    Situational Awareness & The interface must maintain high operator \ac{SA} \\
    Usability & The system must demonstrate good usability \\
    User Acceptance & The interface must achieve high user acceptance \\
    Attention & The system must effectively direct user attention to relevant information \\
    Capacity & The interface must not overwhelm the operator's mental and physical capacities \\
    \bottomrule
    \end{tabular}
    \caption{Requirements for the Interface}
    \label{table:interface_requirements}
    \end{table}
Within the scope of this thesis, we focus specifically on \ac{SA}, capacity (examined through cognitive load), and the combined aspects of features and information presentation on quantifiable aspects. While the remaining requirements (usability, user acceptance, and attention) are crucial for teleoperation interfaces, they fall outside the scope of our analysis.

Additionally, Georg et al. emphasize system-level requirements that are crucial for practical implementation \cite{Georg}:
\begin{table}[h!]
    \centering
    \begin{tabular}{@{}p{4cm}p{10cm}@{}}
    \toprule
    \textbf{Category} & \textbf{Description} \\
    \midrule
    Scalability & The system should be cost-effective and be able to efficiently handle multiple vehicles \\
    Adaptability & The interface should accommodate different vehicles and scenarios \\
    Data Management & Only necessary sensor data should be transmitted to minimize bandwidth usage \\
    Multi-modal Support & The system should support both conventional displays and head-mounted displays \\
    \bottomrule
    \end{tabular}
    \caption{System Requirements for Scalability and Adaptability}
    \label{table:scalability_adaptability}
    \end{table}
These requirements were also considered throughout the development phase and are discussed in the later sections.

After the requirements phase, user-centered design process involves:

\paragraph{Design Solution Development:} Creating interfaces that meet the specified requirements through iterative prototyping and refinement. This includes implementing visualization methods that effectively combine sensor data and perception outputs.
This aspect of the process is discussed within chapter \ref{chapter:methodology}.
\paragraph{Evaluation:} Testing the design solutions against the requirements through user studies and expert evaluations. This involves measuring \ac{SA}, cognitive load, and task performance using standardized metrics and assessment tools. And This part of the process is introduced in section \ref{section:evaluationmethods} and analyzed in more detail in chapter \ref{chapter:userstudy}.

This systematic approach ensures that the resulting interface design meets technical requirements and effectively supports operator needs and capabilities in teleoperation scenarios.


\section{Evaluation Methods}\label{section:evaluationmethods}
The evaluation of teleoperation interfaces requires a systematic approach to measure operator performance, \ac{SA}, and cognitive load. This section discusses various evaluation methods, focusing on \ac{SA}, cognitive load and interface performance evaluations.
\subsection{Situational Awareness in Teleoperation}\label{subsection:situationawareness}
As discussed in Section \ref{subsection:humanfactors}, \ac{SA} is critical in teleoperation scenarios, mainly due to the physical separation between the operator and the vehicle. Operators rely entirely on sensor data and interface representations to understand the remote environment, making the quality and presentation of this information crucial for effective operation \cite{Gnatzig}. As Endsley explains, "Even the best-trained decision makers will make the wrong decisions if they have inaccurate or incomplete SA" \cite{endsley1995toward}.

Two prominent methods are commonly used to evaluate \ac{SA} in teleoperation tasks: the \ac{SAGAT} and the \ac{SART}. \ac{SAGAT}, developed by Endsley \cite{endsley1988sagat}, is an objective measure that involves freezing a simulation at random intervals and querying operators about their perception and understanding of the situation. During these pauses, displays are blanked, and operators are asked specific questions about elements in the environment, such as object locations or potential risks \cite{endsley2000direct}. This method allows for detailed assessment across all three levels of \ac{SA}: perception (Level 1), comprehension (Level 2), and projection (Level 3).

In contrast, SART is a subjective post-trial rating technique where operators self-assess their \ac{SA} based on factors like attention demand, attentional supply, and understanding \cite{taylor1990sart}. While \ac{SART} provides insights into operators' confidence levels, it has been criticized for measuring operator's confidence in the \ac{SA} they perceive rather than measuring the actual \ac{SA} \cite{endsley2020review}. This distinction is crucial in teleoperation tasks where overconfidence can lead to errors.

For this thesis, we adopt \ac{SAGAT} as our primary evaluation method for \ac{SA}. \ac{SAGAT} offers several advantages over SART:
\begin{itemize}
    \item It provides objective measurements of all three levels of \ac{SA}.
    \item It has been shown to be more sensitive to experimental differences than post-trial questionnaires \cite{endsley2000direct}.
    \item It allows for detailed analysis of specific elements relevant to perception modification tasks.
\end{itemize}
By using \ac{SAGAT}, we aim to understand how different visualization approaches impact operator \ac{SA} during teleoperation.

\subsection{Cognitive Load and Information Processing}\label{subsection:cognitiveload}
Cognitive load is critical in teleoperation, as operators must process large amounts of information from multiple sources while making timely decisions. High cognitive load can impair performance, leading to slower reaction times, reduced \ac{SA}, and increased error rates \cite{Kettwich}. Conversely, underloading the operator can reduce vigilance and engagement, which may also negatively affect task performance \cite{mutzenich2021updating}. Therefore, designing interfaces that balance cognitive demands is crucial for effective teleoperation.

The \ac{NASA-TLX} is one of the most widely used tools for evaluating cognitive load in teleoperation scenarios. \ac{NASA-TLX} is a subjective workload assessment tool that measures an operator's perceived workload across six dimensions: mental demand (cognitive effort required), physical demand (physical exertion involved), temporal demand (time pressure experienced), performance (self-assessment of task success), effort (overall exertion required), and frustration (emotional stress or annoyance) \cite{hart1988development}. These dimensions provide a comprehensive understanding of the cognitive demands placed on operators during teleoperation tasks.

Compared to alternative methods for measuring cognitive load, \ac{NASA-TLX} offers several advantages. Physiological measures such as heart rate variability or pupil dilation provide objective data but require specialized equipment and are sensitive to external factors like stress or fatigue \cite{wickens2008multiple}. Secondary task performance methods involve measuring performance on a secondary task while completing the primary task, offering indirect insights into cognitive load but potentially interfering with the primary task. In contrast, \ac{NASA-TLX} is simple to administer and captures multiple workload dimensions without requiring additional hardware or introducing interference.

While \ac{NASA-TLX} has traditionally been used in its full form with weighted scores for each dimension, simplified versions have been developed to reduce the time required for administration and analysis \cite{hart2006nasa}. For this thesis, we adopt the simplified, unweighted version of \ac{NASA-TLX} to streamline its integration into our user studies while retaining its ability to capture critical aspects of cognitive workload. This approach aligns with the practical constraints of teleoperation scenarios and ensures minimal disruption to operator tasks during evaluations.

Using \ac{NASA-TLX} as our primary tool for assessing cognitive load, we aim to gain valuable insights into how different visualization approaches impact operator mental workload in perception modification tasks
\subsection{Feature and Information Evaluation}
In evaluating the features and information presentation of teleoperation interfaces, we adopt a subjective approach that involves gathering insights from users with expertise in autonomous driving. This approach ensures that the evaluation is grounded in practical experience and industry knowledge, providing valuable feedback on the completeness and effectiveness of the interface features.

Kettwich et al. demonstrated the value of expert evaluations in their study of teleoperation interfaces for highly automated shuttles, where control center professionals assessed the interface's usability, \ac{SA}, and feature completeness \cite{Kettwich}. Similarly, Georg et al. emphasize that domain experts are crucial for evaluating teleoperation systems to ensure that the design meets real-world operational needs \cite{Georg}.

\acp{KPI} for evaluating teleoperation interfaces include feature completeness, information accessibility, and system responsiveness \cite{Brecht}. These \acp{KPI} help ensure that the interface provides all necessary features for effective teleoperation, including monitoring automation, providing disturbance information, and supporting operators in resolving issues.

\section{Summary}
The literature review has explored various aspects of teleoperation interfaces for \acp{AV}, from fundamental concepts to specific visualization approaches. This comprehensive analysis reveals the critical role of effective human-machine interfaces in ensuring safe and efficient teleoperation, particularly for perception modification tasks.
\subsection{Requirements}

The requirements for the visualization interface were derived through a comprehensive analysis of the literature and existing approaches, as outlined in earlier sections of this thesis. The requirements stem from three main sources:

First, the critical requirements for teleoperation interfaces established by Kettwich, et al. \cite{Kettwich} provide the foundation for our interface design requirements, as summarized in Table \ref{table:interface_requirements}. These requirements ensure that the interface effectively supports teleoperation tasks while maintaining usability and safety.

Second, the system-level requirements defined by Georg, et al. \cite{Georg}. address practical implementation considerations, as shown in Table \ref{table:scalability_adaptability}. These requirements were instrumental in shaping our technical approach and ensuring system scalability.

Third, our analysis of existing visualization approaches and teleoperation challenges informed additional requirements specific to perception modification tasks. This includes insights from studies on \ac{SA} in teleoperation [Section \ref{subsection:situationawareness}], cognitive load management [Section \ref{subsection:cognitiveload}], and the technical challenges of integrating multi-sensor data [Section \ref{subsection:challengesmultisensor}].

The synthesis of these sources led to a comprehensive set of requirements categorized into four key areas: \ac{SA}, Cognitive Load, Features \& Information, and Technical considerations. These requirements are summarized in Table \ref{table:requirements}, which serves as the primary reference for our interface development and evaluation process.

By synthesizing findings from these sections and visual references, the resulting requirements ensure that the interface design aligns with both theoretical insights and practical considerations for teleoperation tasks.
\newcounter{reqcounter}
\begin{table}[h!]
    \centering
    \begin{tabular}{@{}p{4cm}p{10cm}@{}}
    \toprule
    \textbf{Category} & \textbf{Requirement Description} \\
    \midrule
    \textbf{Situational Awareness} &
    \begin{enumerate}[label=\arabic*., itemsep=0pt, topsep=0pt, leftmargin=*]
        \setcounter{enumi}{\value{reqcounter}} % Resume numbering from custom counter
        \item The interface must provide comprehensive environmental perception data to enable operators to develop and maintain high \ac{SA}.
        \item Visualization methods must effectively combine multiple sensor data streams into a coherent representation.
        \setcounter{reqcounter}{\value{enumi}} % Save current counter value
    \end{enumerate} \\
    \midrule
    \textbf{Cognitive Load} &
    \begin{enumerate}[label=\arabic*., itemsep=0pt, topsep=0pt, leftmargin=*]
        \setcounter{enumi}{\value{reqcounter}} % Resume numbering from custom counter
        \item The interface must balance information presentation to avoid overwhelming operators.
        \item Visualization approaches should minimize the mental effort required for integrating different data sources.
        \setcounter{reqcounter}{\value{enumi}} % Save current counter value
    \end{enumerate} \\
    \midrule
    \textbf{Feature \& Information} &
    \begin{enumerate}[label=\arabic*., itemsep=0pt, topsep=0pt, leftmargin=*]
        \setcounter{enumi}{\value{reqcounter}} % Resume numbering from custom counter
        \item The interface must provide the necessary features for monitoring automation and perception of data.
        \item Visualization methods must effectively present both raw sensor data and processed perception outputs.
        \item The system should support perception modification tasks through intuitive interaction methods.
        \setcounter{reqcounter}{\value{enumi}} % Save current counter value
    \end{enumerate} \\
    \midrule
    \textbf{Technical} &
    \begin{enumerate}[label=\arabic*., itemsep=0pt, topsep=0pt, leftmargin=*]
        \setcounter{enumi}{\value{reqcounter}} % Resume numbering from custom counter
        \item Real-time visualization performance to support immediate operator response.
        \item Efficient data transmission to work within network bandwidth constraints.
        \item Robust handling of various environmental conditions and sensor configurations.
        \setcounter{reqcounter}{\value{enumi}} % Save current counter value
    \end{enumerate} \\
    \bottomrule
    \end{tabular}
    \caption{Requirements for the Designed \ac{HMI}}
    \label{table:requirements}
    \end{table}
These requirements will guide the development and evaluation of our visualization approaches, ensuring that the resulting interface effectively supports teleoperation tasks while maintaining high usability and performance standards.
\subsection{Research Gap}

While teleoperation has been extensively studied as a fallback solution for \acp{AV}, limited research focuses explicitly on perception modification interfaces and their effectiveness in real-world scenarios \cite{Georg}. Existing studies have primarily concentrated on direct control or remote assistance concepts, leaving a significant gap in understanding how different visualization approaches affect operator performance in perception modification tasks.

Current research lacks comprehensive user studies comparing different visualization methods for perception data presentation. Although some studies have evaluated individual aspects of teleoperation interfaces \cite{Kettwich}, there is insufficient evidence to determine which visualization approaches are most effective for perception modification tasks. This gap is particularly notable in the context of integrating multiple data streams (2D camera feeds, 3D LiDAR data, and perception outputs) into a unified display.

This thesis addresses these gaps through two main research objectives. First, we aim to develop and evaluate an ideal implementation of our "Integrated View" approach, which combines raw sensor data and perception outputs in a unified visualization. This involves investigating how different visualization techniques affect operator \ac{SA} and cognitive load. Second, we assess how closely our current implementation approaches this ideal, identifying technical limitations and areas for improvement.

Through user studies and performance evaluations, we seek to provide empirical evidence for the effectiveness of different visualization approaches in perception modification tasks. The findings will contribute to the understanding of teleoperation interface design and lay the groundwork for future research in this field, particularly in developing more advanced visualization methods and improving operator performance in perception modification tasks.


